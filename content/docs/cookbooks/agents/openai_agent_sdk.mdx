---
title: SUTRA with OpenAI Agents SDK
---

<div style={{ display: "flex", alignItems: "center", gap: "40px" }}>
  <img src="/sutra.png" width="150" alt="SUTRA" />
  <img
    src="https://static.vecteezy.com/system/resources/previews/021/059/827/non_2x/chatgpt-logo-chat-gpt-icon-on-white-background-free-vector.jpg"
    width="150"
  />
</div>
## SUTRA by TWO Platforms SUTRA is a family of large multi-lingual language
(LMLMs) models pioneered by Two Platforms. SUTRAâ€™s dual-transformer approach
extends the power of both MoE and Dense AI language model architectures,
delivering cost-efficient multilingual capabilities for over 50+ languages. It
powers scalable AI applications for conversation, search, and advanced
reasoning, ensuring high-performance across diverse languages, domains and
applications.

## OpenAI Agents SDK

The OpenAI Agents SDK enables you to build agentic AI apps in a
lightweight, easy-to-use package with very few abstractions. It's a
production-ready upgrade of our previous experimentation for agents,
Swarm..

## Get Your API Keys

Before you begin, make sure you have:

1.  A SUTRA API key (Get yours at [TWO AI's SUTRA API
    page](https://www.two.ai/sutra/api))
2.  Basic familiarity with Python and Jupyter notebooks

This notebook is designed to run in Google Colab, so no local Python
installation is required.

## SUTRA Using OpenAI Agents SDK

### Install Required Dependencies

```python
!pip install "openai-agents[litellm]"
```

### Setup API Keys

```python
import os
from google.colab import userdata

# Set the API key from Colab secrets
os.environ["SUTRA_API_KEY"] = userdata.get("SUTRA_API_KEY")
```

### Configuring Logging to Silence OpenAI Agent Warnings

```python
import logging
logging.getLogger("openai.agents").setLevel(logging.ERROR)
```

### Initializing and Executing a LiteLLM-Based Agent with SUTRA

````python
colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="biPIsngSOd5r" outputId="6fe28960-9efa-46ef-bcd9-bcb197670a92">

```python
import nest_asyncio
import asyncio
import os
from agents import Agent, Runner
from agents.extensions.models.litellm_model import LitellmModel

# Patch the running event loop
nest_asyncio.apply()

# Main async logic
async def main():
    agent = Agent(
        name="Sutra Agent",
        instructions="You are a helpful assistant that responds in Hindi.",
        model=LitellmModel(
            model="openai/sutra-v2",
            api_key=os.environ.get("SUTRA_API_KEY"),
            base_url="https://api.two.ai/v2"
        )
    )

    result = await Runner.run(agent, "à¤‡à¤•à¥à¤µà¤¿à¤Ÿà¥€ à¤•à¥à¤¯à¤¾ à¤¹à¥‹à¤¤à¤¾ à¤¹à¥ˆ?")
    print(result.final_output)

# Run the async function safely
await main()
````

### Streaming Response

```python
import asyncio
import os
from openai.types.responses import ResponseTextDeltaEvent
from agents import Agent, Runner
from agents.extensions.models.litellm_model import LitellmModel

async def main():
    # Create an Agent using Sutra (via LiteLLM)
    agent = Agent(
        name="Sutra Stream Agent",
        instructions="à¤¤à¥à¤® à¤à¤• à¤¸à¤¹à¤¾à¤¯à¤• à¤¹à¥‹ à¤œà¥‹ à¤¹à¤¿à¤‚à¤¦à¥€ à¤®à¥‡à¤‚ à¤œà¤µà¤¾à¤¬ à¤¦à¥‡à¤¤à¤¾ à¤¹à¥ˆà¥¤",  # Instructions in Hindi
        model=LitellmModel(
            model="openai/sutra-v2",
            api_key=os.environ.get("SUTRA_API_KEY"),
            base_url="https://api.two.ai/v2"
        ),
    )

    # Start streaming the response
    result = Runner.run_streamed(agent, input="à¤­à¤¾à¤°à¤¤ à¤•à¤¾ à¤‡à¤¤à¤¿à¤¹à¤¾à¤¸ à¤¬à¤¤à¤¾à¤‡à¤à¥¤")

    print("ðŸŸ¢ Streaming started...\n")
    async for event in result.stream_events():
        # Print LLM tokens as they're streamed
        if event.type == "raw_response_event" and isinstance(event.data, ResponseTextDeltaEvent):
            print(event.data.delta, end="", flush=True)

    print("\nâœ… Streaming complete.")

# ðŸ” Run in a proper async environment
await main()
```

8### Define Weather Tool

```python
from agents import function_tool

@function_tool
def get_weather(city: str) -> str:
    """Get weather details for a given city."""
    return f"{city} à¤®à¥‡à¤‚ à¤®à¥Œà¤¸à¤® à¤¬à¤¹à¥à¤¤ à¤¹à¥€ à¤¸à¥à¤¹à¤¾à¤µà¤¨à¤¾ à¤¹à¥ˆà¥¤"
```

### Define User Info Tool

```python
from typing import Any
from pydantic import BaseModel
from agents import FunctionTool, RunContextWrapper

class UserInfo(BaseModel):
    name: str
    age: int

async def process_user(ctx: RunContextWrapper[Any], args: str) -> str:
    data = UserInfo.model_validate_json(args)
    return f"{data.name} à¤•à¥€ à¤‰à¤®à¥à¤° {data.age} à¤µà¤°à¥à¤· à¤¹à¥ˆà¥¤"

process_user_tool = FunctionTool(
    name="process_user_info",
    description="à¤‰à¤ªà¤¯à¥‹à¤—à¤•à¤°à¥à¤¤à¤¾ à¤•à¥€ à¤œà¤¾à¤¨à¤•à¤¾à¤°à¥€ à¤•à¥‹ à¤¸à¤‚à¤¸à¤¾à¤§à¤¿à¤¤ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆà¥¤",
    params_json_schema=UserInfo.model_json_schema(),
    on_invoke_tool=process_user
)
```

### Translation Agent Setup

```python
from agents import Agent

hindi_translator = Agent(
    name="Hindi Translator",
    instructions="à¤†à¤ª à¤‰à¤ªà¤¯à¥‹à¤—à¤•à¤°à¥à¤¤à¤¾ à¤•à¥‡ à¤¸à¤‚à¤¦à¥‡à¤¶ à¤•à¥‹ à¤¹à¤¿à¤‚à¤¦à¥€ à¤®à¥‡à¤‚ à¤…à¤¨à¥à¤µà¤¾à¤¦ à¤•à¤°à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤",
)

# Turn agent into a tool
translator_tool = hindi_translator.as_tool(
    tool_name="translate_to_hindi",
    tool_description="à¤¸à¤‚à¤¦à¥‡à¤¶ à¤•à¥‹ à¤¹à¤¿à¤‚à¤¦à¥€ à¤®à¥‡à¤‚ à¤…à¤¨à¥à¤µà¤¾à¤¦ à¤•à¤°à¥‡à¤‚à¥¤"
)
```

### Run Multi-Tool Sutra Agent

```python
import nest_asyncio
import asyncio
import os
from agents import Agent, Runner
from agents.extensions.models.litellm_model import LitellmModel

nest_asyncio.apply()

async def main():
    agent = Agent(
        name="Sutra Agent",
        instructions="à¤†à¤ª à¤à¤• à¤¸à¤¹à¤¾à¤¯à¤• à¤¹à¥ˆà¤‚ à¤œà¥‹ à¤¹à¤¿à¤‚à¤¦à¥€ à¤®à¥‡à¤‚ à¤œà¤µà¤¾à¤¬ à¤¦à¥‡à¤¤à¤¾ à¤¹à¥ˆà¥¤",
        model=LitellmModel(
            model="openai/sutra-v2",
            api_key=os.environ["SUTRA_API_KEY"],
            base_url="https://api.two.ai/v2"
        ),
        tools=[get_weather, process_user_tool, translator_tool],
    )

    result = await Runner.run(agent, "à¤®à¥à¤à¥‡ à¤œà¤¯à¤ªà¥à¤° à¤•à¥‡ à¤®à¥Œà¤¸à¤® à¤•à¥‡ à¤¬à¤¾à¤°à¥‡ à¤®à¥‡à¤‚ à¤¬à¤¤à¤¾à¤‡à¤à¥¤")
    print(result.final_output)

await main()
```

### Guardrails via OpenAI Agents SDK

```python
import nest_asyncio
import asyncio
import os
from pydantic import BaseModel
from agents import (
    Agent,
    Runner,
    function_tool,
    GuardrailFunctionOutput,
    RunContextWrapper,
    InputGuardrailTripwireTriggered,
    input_guardrail,
    TResponseInputItem,
)
from agents.extensions.models.litellm_model import LitellmModel

# Patch event loop for Jupyter or other async environments
nest_asyncio.apply()


# Define the guardrail output model
class MathHomeworkOutput(BaseModel):
    is_math_homework: bool
    reasoning: str

# Setup guardrail agent - MUST output strict JSON only!
guardrail_agent = Agent(
    name="Math Homework Guardrail",
    instructions=(
        "You are a JSON API. Determine if the input asks for math homework help.\n"
        "Output ONLY a JSON object with fields:\n"
        "- is_math_homework (boolean)\n"
        "- reasoning (string)\n"
        "Do NOT output any extra text."
    ),
    output_type=MathHomeworkOutput,
    model=LitellmModel(
        model="openai/sutra-v2",
        api_key=os.environ.get("SUTRA_API_KEY"),
        base_url="https://api.two.ai/v2",

    ),
)

# Guardrail function using input_guardrail decorator
@input_guardrail
async def math_guardrail(
    ctx: RunContextWrapper[None],
    agent: Agent,
    input: str | list[TResponseInputItem]
) -> GuardrailFunctionOutput:
    result = await Runner.run(guardrail_agent, input, context=ctx.context)
    return GuardrailFunctionOutput(
        output_info=result.final_output,
        tripwire_triggered=result.final_output.is_math_homework,
    )

# Main agent setup with Sutra model and input guardrail
main_agent = Agent(
    name="Sutra Agent",
    instructions="You are a helpful assistant that responds in Hindi.",
    model=LitellmModel(
        model="openai/sutra-v2",
        api_key=os.environ.get("SUTRA_API_KEY"),
        base_url="https://api.two.ai/v2",
    ),
    tools=[get_weather],
    input_guardrails=[math_guardrail],
)
```

### Guardrail Blocking vs Allowing Queries

```python
async def main():
    # Example 1: Math homework input (should trigger guardrail)
    try:
        response = await Runner.run(main_agent, "à¤®à¥à¤à¥‡ 2x + 3 = 11 à¤•à¤¾ à¤¹à¤² à¤¨à¤¿à¤•à¤¾à¤²à¤¨à¥‡ à¤®à¥‡à¤‚ à¤®à¤¦à¤¦ à¤•à¤°à¥‡à¤‚à¥¤")
        print("Example 1 - Agent response:", response.final_output)
    except InputGuardrailTripwireTriggered:
        print("Example 1 - âš ï¸ Guardrail triggered: Math homework detected! Agent run stopped.")

    # Example 2: Non-math input (should NOT trigger guardrail)
    try:
        response = await Runner.run(main_agent, "à¤®à¥à¤à¥‡ à¤¦à¤¿à¤²à¥à¤²à¥€ à¤•à¥‡ à¤®à¥Œà¤¸à¤® à¤•à¥‡ à¤¬à¤¾à¤°à¥‡ à¤®à¥‡à¤‚ à¤¬à¤¤à¤¾à¤“à¥¤")
        print("Example 2 - Agent response:", response.final_output)
    except InputGuardrailTripwireTriggered:
        print("Example 2 - âš ï¸ Guardrail triggered unexpectedly!")

if __name__ == "__main__":
    asyncio.run(main())
```

### Orchestrating multiple agents

```python
import nest_asyncio
import asyncio
import os
from agents import Agent, Runner, function_tool
from agents.extensions.models.litellm_model import LitellmModel

nest_asyncio.apply()

# Function tool example (optional)
@function_tool
def search_web(query: str) -> str:
    # Placeholder: pretend to search web and return snippet
    return f"Search results for '{query}': Important info about {query}."

# Setup Research Agent
research_agent = Agent(
    name="Research Agent",
    instructions="You research topics and return concise summaries.",
    model=LitellmModel(
        model="openai/sutra-v2",
        api_key=os.environ.get("SUTRA_API_KEY"),
        base_url="https://api.two.ai/v2",
    ),
    tools=[search_web],
)

# Setup Writer Agent
writer_agent = Agent(
    name="Writer Agent",
    instructions="You write a blog post based on the provided research notes.",
    model=LitellmModel(
        model="openai/sutra-v2",
        api_key=os.environ.get("SUTRA_API_KEY"),
        base_url="https://api.two.ai/v2",
    ),
)

# Orchestrator Agent: will delegate via handoffs
orchestrator_agent = Agent(
    name="Orchestrator Agent",
    instructions=(
        "You plan a blog post workflow. "
        "First, delegate research tasks to the Research Agent, "
        "then delegate writing to the Writer Agent using research output."
    ),
    model=LitellmModel(
        model="openai/sutra-v2",
        api_key=os.environ.get("SUTRA_API_KEY"),
        base_url="https://api.two.ai/v2",
    ),
    handoffs=[research_agent, writer_agent],  # allows delegating tasks
)

# -------- ORCHESTRATION VIA CODE --------
async def orchestrate_via_code(topic: str):
    # Step 1: Run Research Agent
    research_result = await Runner.run(research_agent, topic)
    research_notes = research_result.final_output
    print("\n[Research Agent Output]:", research_notes)

    # Step 2: Run Writer Agent with research notes as input
    writer_result = await Runner.run(writer_agent, research_notes)
    blog_post = writer_result.final_output
    print("\n[Writer Agent Output]:", blog_post)

# -------- ORCHESTRATION VIA LLM HANDOFFS --------
async def orchestrate_via_llm(topic: str):
    # Just call orchestrator agent with the topic and it handles handoffs internally
    result = await Runner.run(orchestrator_agent, topic)
    print("\n[Orchestrator Agent Output]:", result.final_output)

async def main():
    topic = "Climate change impact on agriculture"

    print("===== Orchestration via code =====")
    await orchestrate_via_code(topic)

    print("\n===== Orchestration via LLM handoffs =====")
    await orchestrate_via_llm(topic)

if __name__ == "__main__":
    asyncio.run(main())
```
