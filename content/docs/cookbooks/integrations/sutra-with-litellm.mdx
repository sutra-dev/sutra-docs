---
title: SUTRA using LiteLLM
---

<div style={{ display: "flex", alignItems: "center", gap: "40px" }}>
  <img src="/sutra.png" width="150" alt="SUTRA" />
  <img
    src="https://pbs.twimg.com/profile_images/1783589223406415872/3KMxGGrF_400x400.jpg"
    width="150"
  />
</div>

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1GXzlWhayCSxjdvLXHvO9otT7LW3tuNvk?usp=sharing)

## SUTRA by TWO Platforms

SUTRA is a family of large multi-lingual language models (LMLMs) pioneered by Two Platforms. SUTRA‚Äôs dual-transformer approach extends the power of both MoE and Dense AI language model architectures, delivering cost-efficient multilingual capabilities for over 50+ languages. It powers scalable AI applications for conversation, search, and advanced reasoning, ensuring high-performance across diverse languages, domains and applications.

## LiteLLM üöÖ

LiteLLM simplifies access to 100+ large language models (LLMs) with a unified API. It enables easy model integration, spend tracking, rate-limiting, fallbacks, and observability‚Äîhelping developers manage LLMs like OpenAI, Anthropic, Groq, Cohere, Google, and more from a single interface.

## Get Your API Keys

Before you begin, make sure you have:

1.  A SUTRA API key (Get yours at [TWO AI's SUTRA API
    page](https://www.two.ai/sutra/api))
2.  Basic familiarity with Python and Jupyter notebooks

This notebook is designed to run in Google Colab, so no local Python
installation is required.

### Sutra using LiteLLM üöÖ

###Install Requirements

```python
# Install required packages
!pip install litellm
```

### Setup API Keys

```python
import os
from google.colab import userdata

# Set the API key from Colab secrets
os.environ["SUTRA_API_KEY"] = userdata.get("SUTRA_API_KEY")
```

### Initialize Sutra Model via LiteLLM üöÖ:

```python
import os
import litellm

# Environment variable for better security
api_key = os.environ.get("SUTRA_API_KEY")

# Call the Sutra LLM via LiteLLM
response = litellm.completion(
    model="openai/sutra-v2",
    api_key=api_key,
    api_base="https://api.two.ai/v2",
    messages=[
        {
            "role": "user",
            "content": "‡§Æ‡•Å‡§ù‡•á ‡§Æ‡§Ç‡§ó‡§≤ ‡§ó‡•ç‡§∞‡§π ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç 5 ‡§™‡•à‡§∞‡§æ‡§ó‡•ç‡§∞‡§æ‡§´ ‡§¶‡•Ä‡§ú‡§ø‡§è"
        }
    ],
    temperature=0.7,
    max_tokens=500
)

print(response['choices'][0]['message']['content'])
```

### Example 1: Streaming Response :

```python
response = litellm.completion(
    model="openai/sutra-v2",
    api_key=api_key,
    api_base="https://api.two.ai/v2",
    messages=[
        {
            "role": "user",
            "content": "Explain quantum computing in simple terms"
        }
    ],
    temperature=0.8,
    max_tokens=1024,
    stream=True
)

for chunk in response:
    if chunk.choices[0].delta.content is not None:
        print(chunk.choices[0].delta.content, end='')
```

### Example 2: Code Generation :

```python
response = litellm.completion(
    model="openai/sutra-v2",
    api_key=api_key,
    api_base="https://api.two.ai/v2",
    messages=[
        {
            "role": "user",
            "content": "Write a Python function to find the factorial of a number using recursion"
        }
    ],
    temperature=0.5,
    max_tokens=700
)

print(response['choices'][0]['message']['content'])
```

### Example 3: Multilingual Support :

```python
languages = ["Hindi", "Tamil", "Bengali", "Spanish", "Arabic"]
questions = [
    "‡§≠‡§æ‡§∞‡§§‡•Ä‡§Ø ‡§∏‡•ç‡§µ‡§§‡§Ç‡§§‡•ç‡§∞‡§§‡§æ ‡§∏‡§Ç‡§ó‡•ç‡§∞‡§æ‡§Æ ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§¨‡§§‡§æ‡§á‡§è",
    "‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç ‡Æ®‡Ææ‡Æü‡ØÅ ‡Æ™‡Æ±‡Øç‡Æ±‡Æø ‡Æé‡Æ¥‡ØÅ‡Æ§‡ØÅ‡Æô‡Øç‡Æï‡Æ≥‡Øç",
    "‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶∏‡¶æ‡¶π‡¶ø‡¶§‡ßç‡¶Ø‡ßá‡¶∞ ‡¶á‡¶§‡¶ø‡¶π‡¶æ‡¶∏ ‡¶ï‡ßÄ?",
    "¬øQu√© es la inteligencia artificial?",
    "ŸÖÿß ŸáŸà ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸäÿü"
]
for lang, q in zip(languages, questions):
    print(f"\n--- {lang} ---")
    response = litellm.completion(
        model="openai/sutra-v2",
        api_key=api_key,
        api_base="https://api.two.ai/v2",
        messages=[{"role": "user", "content": q}],
        temperature=0.6,
        max_tokens=500
    )
    print(response['choices'][0]['message']['content'])
```

### Example 4:Multilingual Translation :

```python
translations = [
    ("English", "Hindi", "Artificial Intelligence is transforming every industry."),
    ("English", "Japanese", "The global economy is experiencing rapid changes."),
    ("English", "Arabic", "Climate change is a pressing global issue."),
    ("English", "Tamil", "Education is the foundation of a strong society."),
    ("English", "French", "Healthcare innovation is advancing at an incredible pace.")
]

for source_lang, target_lang, text in translations:
    print(f"\n=== {source_lang} ‚Üí {target_lang} ===")
    prompt = f"Translate the following {source_lang} sentence into {target_lang}:\n\n{text}"

    response = litellm.completion(
        model="openai/sutra-v2",
        api_key=api_key,
        api_base="https://api.two.ai/v2",
        messages=[
            {"role": "user", "content": prompt}
        ],
        temperature=0.3,
        max_tokens=200
    )

    print("Original:", text)
    print("Translated:", response['choices'][0]['message']['content'])
```

### Building a Simple Chatbot with LiteLLM üöÖ

```python
import os
import litellm

# Basic chat loop with history
print("Chatbot (Sutra): Hello! Type 'exit' to end the conversation.\n")

chat_history = []

while True:
    user_input = input("You: ")

    if user_input.lower() == "exit":
        print("Chatbot: Goodbye! üëã")
        break

    chat_history.append({"role": "user", "content": user_input})

    try:
        response = litellm.completion(
            model="openai/sutra-v2",
            api_key=api_key,
            api_base="https://api.two.ai/v2",
            messages=chat_history,
            temperature=0.7,
            max_tokens=500,
        )

        reply = response["choices"][0]["message"]["content"]
        print("Chatbot:", reply)

        chat_history.append({"role": "assistant", "content": reply})

    except Exception as e:
        print("Error:", str(e))
```
