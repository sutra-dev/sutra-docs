---
title: SUTRA with LangChain
---

<div className="flex items-center gap-16">

<img
  src="https://play-lh.googleusercontent.com/_O9p4Z4yucA2NLmZBu9mTJCuBwXeT9NcbtrDN6I8gKlkIPRySV0adOmbyipjSj9Gew"
  width="130"
  alt="two ai"
/>
<img
  src="https://registry.npmmirror.com/@lobehub/icons-static-png/1.45.0/files/light/langchain.png"
  width="120"
/>

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1OYpQFo88aLWbv0D1_DA2j4HZGRxTjRcK?usp=sharing)

<div>
  <h2>SUTRA by TWO Platforms</h2>
  <p>SUTRA is a family of large multi-lingual language (LMLMs) models pioneered by Two Platforms. SUTRA‚Äôs dual-transformer approach extends the power of both MoE and Dense AI language model architectures, delivering cost-efficient multilingual capabilities for over 50+ languages. It powers scalable AI applications for conversation, search, and advanced reasoning, ensuring high-performance across diverse languages, domains and applications.</p>

  <h2>LangChain</h2>
  <p>LangChain is a framework for developing applications powered by large language models (LLMs).</p>
</div>
</div>

#SUTRA using Langchain

## Get Your API Keys

Before you begin, make sure you have:

1. A SUTRA API key (Get yours at [TWO AI's SUTRA API page](https://www.two.ai/sutra/api))
2. Basic familiarity with Python and Jupyter notebooks

This notebook is designed to run in Google Colab, so no local Python installation is required.

###Install Requirements

```python
!pip install -qU langchain_openai langchain_community tavily-python faiss-cpu tiktoken
```

####Setup API Keys

```python
import os
from google.colab import userdata

# Set the API key from Colab secrets
os.environ["SUTRA_API_KEY"] = userdata.get("SUTRA_API_KEY")
os.environ["TAVILY_API_KEY"] = userdata.get("TAVILY_API_KEY")
os.environ["OPENAI_API_KEY"] = userdata.get("OPENAI_API_KEY")
```

### Initialize SUTRA Model via Langchain:

```python
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

# Initialize SUTRA model (OpenAI-compatible)
llm = ChatOpenAI(
    api_key=os.getenv("SUTRA_API_KEY"),
    base_url="https://api.two.ai/v2",  # Replace with actual endpoint
    model="sutra-v2"
)

# Generate a simple chat completion
response = llm.invoke([HumanMessage(content="What is LangChain?")])
print(response.content)
```

### Multilingual capabilities using Langchain

```python
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

# Initialize the ChatOpenAI model
chat = ChatOpenAI(
    api_key=os.getenv("SUTRA_API_KEY"),
    base_url="https://api.two.ai/v2",
    model="sutra-v2",

)

# List of messages in different languages
messages_list = [
    HumanMessage(content="‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å‡∞≤‡±ã ‡∞í‡∞ï ‡∞ï‡∞• ‡∞ö‡±Ü‡∞™‡±ç‡∞™‡±Å?"),                          # Telugu: "Tell a story in Telugu"
    HumanMessage(content="Une histoire en fran√ßais, s'il vous pla√Æt."),      # French: "A story in French, please."
    HumanMessage(content="Por favor, cu√©ntame una historia en espa√±ol."),    # Spanish: "Please tell me a story in Spanish."
    HumanMessage(content="‡§ï‡•É‡§™‡§Ø‡§æ ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§ï‡§π‡§æ‡§®‡•Ä ‡§∏‡•Å‡§®‡§æ‡§á‡§è‡•§"),                     # Hindi: "Please tell a story in Hindi."
    HumanMessage(content="Bitte erz√§hle mir eine Geschichte auf Deutsch.")   # German: "Please tell me a story in German."
]

# Loop through each language request
for msg in messages_list:
    response = chat.invoke([msg])
    print(f"\nPrompt: {msg.content}")
    print(f"Response: {response.content}\n")
```

### Building a Simple Chatbot with Langchain

```python
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

# Initialize the ChatOpenAI model
chat = ChatOpenAI(
    api_key=os.getenv("SUTRA_API_KEY"),
    base_url="https://api.two.ai/v2",
    model="sutra-v2"
)

# Start the chatbot conversation loop
print("Chatbot: Hello! Type 'exit' to end the conversation.\n")

chat_history = []

while True:
    user_input = input("You: ")  # Get user input

    if user_input.lower() == "exit":
        print("Chatbot: Goodbye! üëã")
        break

    # Add user message to chat history
    chat_history.append(HumanMessage(content=user_input))

    # Get response from AI
    response = chat.invoke(chat_history)

    # Print AI response
    print("Chatbot:", response.content)

    # Add AI response to chat history
    chat_history.append(response)
```

###Langchain with the SUTRA model for Document Querying

```python
from langchain_openai import ChatOpenAI
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings

# Use SUTRA for LLM
llm = ChatOpenAI(
    api_key=userdata.get("SUTRA_API_KEY"),
    base_url="https://api.two.ai/v2",
    model="sutra-v2",
)

# Hindi sample text
text = """
SUTRA ‡§¶‡•ã ‡§™‡•ç‡§≤‡•á‡§ü‡§´‡§º‡•â‡§∞‡•ç‡§Æ‡•ç‡§∏ ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§µ‡§ø‡§ï‡§∏‡§ø‡§§ ‡§è‡§ï ‡§¨‡§π‡•Å‡§≠‡§æ‡§∑‡•Ä ‡§¨‡§°‡§º‡•á ‡§≠‡§æ‡§∑‡§æ ‡§Æ‡•â‡§°‡§≤ (LMLM) ‡§™‡§∞‡§ø‡§µ‡§æ‡§∞ ‡§π‡•à‡•§
SUTRA ‡§ï‡•Ä ‡§°‡•Å‡§Ö‡§≤-‡§ü‡•ç‡§∞‡§æ‡§Ç‡§∏‡§´‡•â‡§∞‡•ç‡§Æ‡§∞ ‡§™‡§¶‡•ç‡§ß‡§§‡§ø MoE ‡§î‡§∞ Dense AI ‡§Æ‡•â‡§°‡§≤ ‡§Ü‡§∞‡•ç‡§ï‡§ø‡§ü‡•á‡§ï‡•ç‡§ö‡§∞ ‡§¶‡•ã‡§®‡•ã‡§Ç ‡§ï‡•Ä ‡§∂‡§ï‡•ç‡§§‡§ø ‡§ï‡•ã ‡§¨‡§¢‡§º‡§æ‡§§‡•Ä ‡§π‡•à,
‡§ú‡•ã 50+ ‡§∏‡•á ‡§Ö‡§ß‡§ø‡§ï ‡§≠‡§æ‡§∑‡§æ‡§ì‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§≤‡§æ‡§ó‡§§-‡§ï‡•Å‡§∂‡§≤ ‡§¨‡§π‡•Å‡§≠‡§æ‡§∑‡•Ä ‡§ï‡•ç‡§∑‡§Æ‡§§‡§æ‡§è‡§Ç ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ï‡§∞‡§§‡•Ä ‡§π‡•à‡•§ ‡§Ø‡§π ‡§∏‡§Ç‡§µ‡§æ‡§¶, ‡§ñ‡•ã‡§ú,
‡§î‡§∞ ‡§â‡§®‡•ç‡§®‡§§ ‡§§‡§∞‡•ç‡§ï ‡§ï‡•á ‡§≤‡§ø‡§è ‡§∏‡•ç‡§ï‡•á‡§≤‡•á‡§¨‡§≤ ‡§è‡§Ü‡§à ‡§è‡§™‡•ç‡§≤‡§ø‡§ï‡•á‡§∂‡§® ‡§ï‡•ã ‡§∏‡§ï‡•ç‡§∑‡§Æ ‡§¨‡§®‡§æ‡§§‡§æ ‡§π‡•à, ‡§ú‡•ã ‡§µ‡§ø‡§≠‡§ø‡§®‡•ç‡§® ‡§≠‡§æ‡§∑‡§æ‡§ì‡§Ç, ‡§°‡•ã‡§Æ‡•á‡§® ‡§î‡§∞ ‡§â‡§™‡§Ø‡•ã‡§ó‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§â‡§ö‡•ç‡§ö ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§® ‡§∏‡•Å‡§®‡§ø‡§∂‡•ç‡§ö‡§ø‡§§ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
"""

# Split into chunks
text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
docs = text_splitter.create_documents([text])

# Embedding model
embedding_model = OpenAIEmbeddings(
    model="text-embedding-3-large",
    openai_api_key=os.environ["OPENAI_API_KEY"]
)

# Vector store
vectorstore = FAISS.from_documents(docs, embedding_model)

# RetrievalQA Chain
retriever = vectorstore.as_retriever()
qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)

# Ask question in Hindi
query = "SUTRA ‡§ï‡§ø‡§§‡§®‡•Ä ‡§≠‡§æ‡§∑‡§æ‡§ì‡§Ç ‡§ï‡§æ ‡§∏‡§Æ‡§∞‡•ç‡§•‡§® ‡§ï‡§∞‡§§‡§æ ‡§π‡•à?"
result = qa_chain({"query": query})

# Print result
print("‡§â‡§§‡•ç‡§§‡§∞:", result["result"])
```

### Multilingual Tavily Agent Using SUTRA LMLMs

```python
from langchain.agents import Tool, AgentExecutor, initialize_agent
from langchain.agents.agent_types import AgentType
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

# Step 1: Define SUTRA LLM via LangChain
llm = ChatOpenAI(
    api_key=userdata.get("SUTRA_API_KEY"),
    base_url="https://api.two.ai/v2",
    model="sutra-v2"
)

# Step 2: Setup Tavily search tool
search_tool = TavilySearchResults(max_results=2)

# Step 3: Convert it to LangChain Tool
search = Tool.from_function(
    name="Tavily Search",
    description="Real-time web search tool. ‡§â‡§™‡§Ø‡•ã‡§ó‡§ï‡§∞‡•ç‡§§‡§æ ‡§ï‡•á ‡§™‡•ç‡§∞‡§∂‡•ç‡§®‡•ã‡§Ç ‡§ï‡•á ‡§â‡§§‡•ç‡§§‡§∞ ‡§ñ‡•ã‡§ú‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§µ‡•á‡§¨ ‡§™‡§∞ ‡§ñ‡•ã‡§ú ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§",
    func=search_tool.invoke
)

# Step 4: Initialize Agent
agent = initialize_agent(
    tools=[search],
    llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True
)

# Step 5: Multilingual Query Example
query = "‡§Ü‡§à‡§∏‡•Ä‡§∏‡•Ä 2025 ‡§´‡§æ‡§á‡§®‡§≤ ‡§ï‡§ø‡§∏‡§®‡•á ‡§ú‡•Ä‡§§‡§æ?"  # Hindi: Who Won ICC 2025 Finals?

# Step 6: Invoke agent
response = agent.invoke({"input": query})
print("‡§â‡§§‡•ç‡§§‡§∞:", response["output"])
```
